{
  "query_id": "q4",
  "tier": "easy",
  "strategy": "astar",
  "question": "How does DSPy integrate with LLMs?",
  "execution_time": 10.506568908691406,
  "success": true,
  "steps_taken": 1,
  "final_answer": "DSPy is built **around** large language models (LLMs) rather than on top of them.  In practice the integration works at three levels:\n\n1. **LLM as a first\u2011class resource** \u2013  \n   *DSPy* ships with a thin wrapper class `dspy.LM` that knows how to talk to any text\u2011generation model.  You can point it at OpenAI\u2019s `gpt\u20113.5\u2011turbo`, Anthropic\u2019s `claude\u20112`, a local Ollama model, or any other API that accepts a prompt and returns a completion.  The wrapper is configured once with `dspy.configure(lm=your_lm)` and then becomes the default model for every module in the program.\n\n2. **Declarative modules that invoke the LLM** \u2013  \n   Instead of hand\u2011crafting prompt strings, you declare **signatures** (e.g. `\"question -> answer\"` or a full class\u2011based signature) and then instantiate a **module** such as `dspy.Predict`, `dspy.ChainOfThought`, `dspy.ReAct`, `dspy.Retrieve`, etc.  Each module internally builds the appropriate prompt from the signature, inserts any demonstrations that have been collected, calls the configured `dspy.LM`, and parses the model\u2019s output back into Python objects.  Because the module abstracts the call, the same code works with any underlying LLM without modification.\n\n3. **Automatic optimisation (the compiler/teleprompters)** \u2013  \n   DSPy\u2019s compiler can \u201ccompile\u201d a program by running a **teleprompter** (e.g. `BootstrapFewShot`, `BootstrapFewShotWithRandomSearch`, `MIPROv2`, `BootstrapFinetune`).  During compilation the framework executes the program on a small training set, records the full trace of every LLM call, and then automatically:\n   * generates high\u2011quality few\u2011shot demonstrations,\n   * tweaks the natural\u2011language instructions,\n   * optionally fine\u2011tunes a smaller model (e.g. T5\u2011Large) for a specific module,\n   * selects the best prompt configuration according to a user\u2011provided metric.\n\n   The result is a compiled Python pipeline that still calls the LLM, but now with **self\u2011improving prompts** that are tailored to the task and the chosen model.\n\n### Typical workflow\n\n```python\nimport dspy\n\n# 1\ufe0f\u20e3 Choose and configure the LLM\nllm = dspy.LM(\n    model=\"gpt-3.5-turbo\",          # could be \"claude-2\", \"ollama/llama3.2\", etc.\n    api_key=\"YOUR_API_KEY\",\n    temperature=0.2\n)\ndspy.configure(lm=llm)\n\n# 2\ufe0f\u20e3 Declare the task signature\nqa = dspy.Predict(\"question -> answer\")\n\n# 3\ufe0f\u20e3 Build a pipeline (you can chain modules, add retrieval, etc.)\nclass SimpleRAG(dspy.Module):\n    def __init__(self, k=3):\n        self.retrieve = dspy.Retrieve(k=k)          # uses a retriever tool\n        self.answer   = dspy.ChainOfThought(\"context, question -> answer\")\n    def forward(self, question):\n        ctx = self.retrieve(question).passages\n        return self.answer(context=ctx, question=question)\n\nrag = SimpleRAG()\n\n# 4\ufe0f\u20e3 Optimise the pipeline (optional but where DSPy shines)\ntele = dspy.BootstrapFewShot(metric=dspy.evaluate.answer_exact_match)\ncompiled_rag = tele.compile(rag, trainset=[dspy.Example(question=\"\u2026\", answer=\"\u2026\")])\n```\n\nAfter compilation, `compiled_rag` will automatically generate the best prompt for the retrieval step, the best few\u2011shot examples for the reasoning step, and will invoke the same LLM (`gpt\u20113.5\u2011turbo` in the example) under the hood.  Switching to a different model (e.g., a local Llama\u202f2\u201113b\u2011chat) only requires changing the `dspy.LM` instantiation; the rest of the program remains unchanged.\n\n### Key take\u2011aways\n\n* **LLM\u2011agnostic** \u2013 DSPy treats the model as a plug\u2011in; any model that follows the standard \u201cprompt \u2192 completion\u201d contract can be used.  \n* **Programmatic abstraction** \u2013 Prompt engineering is replaced by declarative signatures and reusable modules, so developers focus on *what* the model should do, not *how* to phrase the prompt.  \n* **Self\u2011improving pipelines** \u2013 The compiler/teleprompters automatically bootstrap and optimise prompts (and optionally fine\u2011tune models), delivering higher reliability and performance without manual prompt tweaking.  \n\nIn short, DSPy integrates with LLMs by wrapping them in a unified Python API, exposing their capabilities through modular, declarative building blocks, and then automatically refining those blocks to produce robust, scalable AI pipelines.",
  "evaluation_score": 9.0,
  "individual_scores": [
    9,
    9,
    9,
    9,
    9
  ],
  "n_evaluations": 5
}